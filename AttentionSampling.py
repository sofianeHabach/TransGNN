import torch as t
import torch.nn as nn
import numpy as np
from scipy.sparse import csr_matrix
from Params import args

class AttentionSampling:
    """
    Module d'√©chantillonnage d'attention (Section 3.2 de l'article)
    Version corrig√©e et optimis√©e
    """
    
    def __init__(self, num_nodes, embedding_dim, sample_size=20, alpha=0.5):
        self.num_nodes = num_nodes
        self.embedding_dim = embedding_dim
        self.sample_size = sample_size
        self.alpha = alpha
        self.attention_samples = None
        self.attention_scores = None
        
    def compute_similarity_matrix(self, embeddings, adj_matrix=None):
        """
        Calcule la matrice de similarit√© selon Eq. 1 et 2 de l'article
        Version optimis√©e pour √©viter out-of-memory
        """
        # √âtape 1: Similarit√© s√©mantique brute (Eq. 1)
        embeddings_normalized = t.nn.functional.normalize(embeddings, p=2, dim=1)
        
        # Pour grands graphes, calculer par batch
        if self.num_nodes > 10000:
            similarity = self._compute_similarity_batched(embeddings_normalized)
        else:
            similarity = t.mm(embeddings_normalized, embeddings_normalized.t())
        
        # √âtape 2: Incorporer pr√©f√©rences des voisins (Eq. 2)
        if adj_matrix is not None and self.alpha > 0:
            # S = S + Œ± * √Ç * S
            # Convertir S en dense si sparse (pour calcul efficace)
            if similarity.is_sparse:
                similarity_dense = similarity.to_dense()
            else:
                similarity_dense = similarity
            
            # Multiplication sparse @ dense
            similarity_propagated = t.sparse.mm(adj_matrix, similarity_dense)
            
            # Ajouter identit√© √† adj_matrix (self-loops)
            # Cr√©er matrice identit√© sparse
            indices = t.arange(self.num_nodes).unsqueeze(0).repeat(2, 1).cuda()
            values = t.ones(self.num_nodes).cuda()
            identity = t.sparse_coo_tensor(indices, values, adj_matrix.shape).cuda()
            
            # √Ç = A + I
            adj_with_self = adj_matrix + identity
            
            # Propager similarit√©
            similarity_propagated = t.sparse.mm(adj_with_self, similarity_dense)
            similarity = similarity_dense + self.alpha * similarity_propagated
        
        return similarity
    
    def _compute_similarity_batched(self, embeddings_normalized, batch_size=1000):
        """
        Calcule similarit√© par batch pour √©conomiser m√©moire
        """
        N = embeddings_normalized.shape[0]
        similarity = t.zeros((N, N), device=embeddings_normalized.device)
        
        for i in range(0, N, batch_size):
            end_i = min(i + batch_size, N)
            batch_i = embeddings_normalized[i:end_i]
            
            # Calculer similarit√© pour ce batch
            sim_batch = t.mm(batch_i, embeddings_normalized.t())
            similarity[i:end_i] = sim_batch
        
        return similarity
    
    def sample_attention_nodes(self, similarity_matrix, exclude_self=True):
        """
        √âchantillonne les top-k n≈ìuds les plus similaires
        Version optimis√©e par batch
        """
        N = similarity_matrix.shape[0]
        k = min(self.sample_size, N - 1) if exclude_self else self.sample_size
        
        if exclude_self:
            # Mettre -inf sur la diagonale
            mask = t.eye(N, dtype=t.bool, device=similarity_matrix.device)
            similarity_matrix = similarity_matrix.masked_fill(mask, float('-inf'))
        
        # √âchantillonner par batch pour √©conomiser m√©moire
        batch_size = 2000
        all_indices = []
        all_scores = []
        
        for start in range(0, N, batch_size):
            end = min(start + batch_size, N)
            batch_sim = similarity_matrix[start:end]
            
            top_scores, top_indices = t.topk(batch_sim, k, dim=1)
            
            # Remplacer -inf par 0
            top_scores = t.where(
                t.isinf(top_scores), 
                t.zeros_like(top_scores), 
                top_scores
            )
            
            all_indices.append(top_indices)
            all_scores.append(top_scores)
        
        self.attention_samples = t.cat(all_indices, dim=0)
        self.attention_scores = t.cat(all_scores, dim=0)
        
        return self.attention_samples, self.attention_scores
    
    def get_attention_samples(self, node_ids):
        """R√©cup√®re √©chantillons pour n≈ìuds sp√©cifiques"""
        if self.attention_samples is None:
            raise ValueError("Appelez d'abord sample_attention_nodes()")
        
        return self.attention_samples[node_ids]
    
    def get_sampled_embeddings(self, embeddings, node_ids):
        """R√©cup√®re embeddings des √©chantillons"""
        if isinstance(node_ids, int):
            node_ids = t.tensor([node_ids], device=embeddings.device)
            squeeze_output = True
        else:
            squeeze_output = False
        
        sample_indices = self.get_attention_samples(node_ids)
        sampled_embeds = embeddings[sample_indices]
        
        if squeeze_output:
            sampled_embeds = sampled_embeds.squeeze(0)
        
        return sampled_embeds
    
    def update_samples(self, embeddings, adj_matrix=None):
        """Recalcule les √©chantillons avec nouveaux embeddings"""
        print("   Recalcul similarit√©...")
        similarity = self.compute_similarity_matrix(embeddings, adj_matrix)
        print("   √âchantillonnage top-k...")
        self.sample_attention_nodes(similarity)
        print("   ‚úì √âchantillons mis √† jour")


def create_attention_sampling(handler, sample_size=20):
    """
    Cr√©e et initialise le module d'attention sampling
    Version simplifi√©e pour √©viter erreurs
    """
    num_nodes = args.user + args.item
    embedding_dim = args.latdim
    
    sampler = AttentionSampling(
        num_nodes=num_nodes,
        embedding_dim=embedding_dim,
        sample_size=sample_size,
        alpha=0.5
    )
    
    print("üîç Initialisation de l'attention sampling...")
    
    # Initialiser avec embeddings al√©atoires
    initial_embeds = t.randn(num_nodes, embedding_dim).cuda()
    
    # Calculer similarit√© SANS adjacence pour la premi√®re fois
    # (√©vite les probl√®mes avec matrice sparse)
    print("   Calcul similarit√© initiale (s√©mantique seulement)...")
    similarity = sampler.compute_similarity_matrix(
        initial_embeds, 
        adj_matrix=None  # Pas d'adjacence pour l'init
    )
    
    print("   √âchantillonnage des top-k n≈ìuds...")
    sampler.sample_attention_nodes(similarity)
    
    print(f"‚úÖ √âchantillonnage cr√©√©: {sample_size} n≈ìuds par n≈ìud central")
    
    return sampler